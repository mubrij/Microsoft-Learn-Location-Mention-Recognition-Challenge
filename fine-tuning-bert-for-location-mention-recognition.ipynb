{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning BERT for Location Mention Recognition\n\nThis notebook demonstrates the process of fine-tuning a BERT model to recognize and categorize location mentions in text using the IDRISI dataset. The task at hand is a type of Named Entity Recognition (NER) where the goal is to identify and classify location names, such as countries, cities, or landmarks, within a given text.\n\nWe utilize the BILOU (Begin, Inside, Last, Outside, Unit) labeling scheme, which provides detailed annotations of entity boundaries. Fine-tuning BERT with these structured labels allows the model to leverage its deep contextual understanding to perform highly accurate token classification, essential for detecting location mentions in diverse textual data.\n\nThe notebook is structured as follows:\n1. **Setup and Installation**: Install and import the necessary libraries.\n2. **Data Ingestion and Preprocessing**: Load the IDRISI dataset and prepare it for modeling, including tokenization and label mapping.\n3. **Modeling Preparation**: Create custom datasets, define label mappings, and set up the BERT model for token classification.\n4. **Fine-Tuning**: Train the BERT model on the labeled data, optimizing for accuracy in location mention recognition.\n5. **Evaluation**: Assess the performance of the fine-tuned model using the Word Error Rate Metric.\n\nBy the end of this notebook, we will have a BERT model that is specifically fine-tuned to recognize and classify location mentions, which can be applied to various real-world applications like geolocation in social media, news analytics, and more.","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install transformers jiwer pandas accelerate -U","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-24T20:16:59.908662Z","iopub.execute_input":"2024-08-24T20:16:59.909065Z","iopub.status.idle":"2024-08-24T20:17:28.157535Z","shell.execute_reply.started":"2024-08-24T20:16:59.909007Z","shell.execute_reply":"2024-08-24T20:17:28.156443Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer, transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.9.6 transformers-4.44.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nfrom collections import Counter\n\nimport pandas as pd\nimport jiwer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:43:53.917447Z","iopub.execute_input":"2024-08-24T20:43:53.918288Z","iopub.status.idle":"2024-08-24T20:43:53.927119Z","shell.execute_reply.started":"2024-08-24T20:43:53.918237Z","shell.execute_reply":"2024-08-24T20:43:53.924443Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Helpers","metadata":{}},{"cell_type":"code","source":"def ingest_idrisi_data(bilou_base_dir='/kaggle/input/idrisi-location-mention/LMR/data/EN/gold-random-bilou/'):\n    sentences, labels = [], []\n    for root, dirs, files in os.walk(bilou_base_dir):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    current_sentence, current_labels = [], []\n                    for line in f:\n                        word_label = line.strip().split()\n                        if len(word_label) == 2:\n                            word, label = word_label\n                            current_sentence.append(word)\n                            current_labels.append(label)\n                        elif len(current_sentence) > 0:\n                            sentences.append(' '.join(current_sentence))\n                            labels.append(','.join(current_labels))\n                            current_sentence, current_labels = [], []\n                    if len(current_sentence) > 0:\n                        sentences.append(' '.join(current_sentence))\n                        labels.append(','.join(current_labels))\n    return pd.DataFrame({'sentence': sentences, 'word_labels': labels})\n\n\n\ndef tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n    tokenized_sentence, labels = [], []\n    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n        tokenized_sentence.extend(tokenized_word)\n        labels.extend([label] * n_subwords)\n    return tokenized_sentence, labels\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        sentence = self.data.sentence[index]  \n        word_labels = self.data.word_labels[index]  \n        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n        \n        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n        labels.insert(0, \"O\")\n        labels.insert(-1, \"O\")\n\n        if len(tokenized_sentence) > self.max_len:\n            tokenized_sentence = tokenized_sentence[:self.max_len]\n            labels = labels[:self.max_len]\n        else:\n            tokenized_sentence += ['[PAD]'] * (self.max_len - len(tokenized_sentence))\n            labels += [\"O\"] * (self.max_len - len(labels))\n\n        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n        label_ids = [label2id[label] for label in labels]\n        \n        return {\n            'input_ids': torch.tensor(ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attn_mask, dtype=torch.long),\n            'labels': torch.tensor(label_ids, dtype=torch.long)\n        }\n    \n    def __len__(self):\n        return len(self.data)\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels.flatten(), preds.flatten(), average='weighted')\n    acc = accuracy_score(labels.flatten(), preds.flatten())\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\ndef infer_on_sentences(sentences, model, tokenizer, max_len=300, with_extra=False):\n    # Put the model in evaluation mode\n    model.eval()\n    \n    results = []\n    extra_results = []\n    \n    for sentence in tqdm(sentences):\n        # Tokenize the sentence and prepare input for the model\n        tokenized_sentence = tokenizer(\n            sentence.split(),\n            is_split_into_words=True,\n            return_offsets_mapping=False,\n            padding='max_length',\n            truncation=True,\n            max_length=max_len,\n            return_tensors=\"pt\"\n        )\n        \n        # Move tensors to the correct device\n        input_ids = tokenized_sentence['input_ids'].to(device)\n        attention_mask = tokenized_sentence['attention_mask'].to(device)\n        \n        # Get predictions\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=2)  # Get the index of the highest logit for each token\n        \n        # Convert predictions to labels\n        pred_labels = [id2label[pred.item()] for pred in predictions[0]]\n        \n        # Get the original tokens from input_ids\n        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n        \n        # Filter out tokens with the 'O' label and concatenate them\n        filtered_tokens = [\n            token for token, label in zip(tokens, pred_labels)\n            if label != 'O' and token not in ['[CLS]', '[SEP]', '[PAD]']\n        ]\n        filtered_labels = [\n            label for token, label in zip(tokens, pred_labels)\n            if label != 'O' and token not in ['[CLS]', '[SEP]', '[PAD]']\n        ]\n        \n        results.append(\" \".join(filtered_tokens))\n        extra_results.append(filtered_labels)\n\n    if with_extra:\n        return results, extra_results\n\n    return results\n\ndef calculate_performance_metric(df, col1='location', col2='prediction'):\n\n    # Function to calculate WER for each row\n    def calculate_wer(row):\n        return jiwer.wer(str(row[col1]), str(row[col2]))\n\n    # Calculate WER for each row\n    df['WER'] = df.apply(calculate_wer, axis=1)\n\n    # Calculate the average WER\n    average_wer = df['WER'].mean()\n\n    return df, average_wer\n\ndef clean_text(text):\n    # Define a dictionary of replacements\n    replacements = {\n        \",\": \" \",\n        \"@\": \"\",\n        \".\": \"\",\n        \";\": \"\",\n        \"-\": \" \",\n        \"_\": \"\",\n        \"#\": \"\",\n        \"##\": \"\"\n    }\n    \n    cleaned_text = text\n    for k, v in replacements.items():\n        cleaned_text = cleaned_text.replace(k, v)\n\n    return cleaned_text\n\ndef clean_prediction(row, raw_prediction_col='prediction_raw'):\n    prediction = row[raw_prediction_col]\n    prediction = prediction.replace(\" ##\", \"\")\n    if prediction.startswith(\"##\"):\n        prediction = \" \".join(prediction.split()[1:])\n\n    cleaned_text = clean_text(row['text'])\n    lower_upper_map = {k.lower(): k for k in cleaned_text.split()}\n\n    for k, v in lower_upper_map.items():\n        prediction = prediction.replace(k, v)\n\n    replacements = {\n        \"U S .\": \"\",\n        \"L . A .\": \"L.A.\",\n        \"P R . P R .\": \"P.R.\",\n        \"N C . N C\": \"N.C.\",\n        \"u . s .\": \"U.S.\",\n        \"s . c .\": \"S.C.\",\n        \"n . c . n . c\": \"N.C.\",\n        \"n . c .\": \"N.C.\",\n        \"d . c .\": \"D.C.\",\n        \"n c . n c\": \"N.C.\",\n        '. r . p . r .': \"P.R.\",\n        \"u s .\": \"U.S.\",\n\n        \" sc\": \"\",\n        \" St\": \"\",\n        \" -\": \"\",\n        \" .\": \"\",\n        \" _\": \"\",\n    }\n    cleaned_prediction = prediction\n    for k, v in replacements.items():\n        cleaned_prediction = cleaned_prediction.replace(k, v)\n\n    prediction_words = cleaned_prediction.split()\n    if len(prediction_words) > 5:\n        cleaned_prediction = Counter(cleaned_prediction.split()).most_common(1)[0][0]\n\n    if len(set(prediction_words)) == 1:\n        cleaned_prediction = prediction_words[0] \n\n    return cleaned_prediction","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:17:47.860065Z","iopub.execute_input":"2024-08-24T20:17:47.860887Z","iopub.status.idle":"2024-08-24T20:17:47.896488Z","shell.execute_reply.started":"2024-08-24T20:17:47.860841Z","shell.execute_reply":"2024-08-24T20:17:47.895515Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"The IDRISI dataset is used because it contains annotated location mentions in text, which is essential for training models to recognize named entities, specifically locations. The data is provided in the BILOU format (Begin, Inside, Last, Outside, Unit), a variant of the IOB (Inside, Outside, Begin) format.","metadata":{}},{"cell_type":"code","source":"# Load IDRISI data\ndata = ingest_idrisi_data()\ndata = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n\n# Split the dataset into training and testing sets\ntrain_size = 0.9999  # High percentage for training data as we've already run a classic split before - this is equivalent to finetuning on the whole dataset\ntrain_dataset = data.sample(frac=train_size, random_state=200)\ntest_dataset = data.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:17:47.898561Z","iopub.execute_input":"2024-08-24T20:17:47.898878Z","iopub.status.idle":"2024-08-24T20:17:48.575417Z","shell.execute_reply.started":"2024-08-24T20:17:47.898845Z","shell.execute_reply":"2024-08-24T20:17:48.574233Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:17:48.576903Z","iopub.execute_input":"2024-08-24T20:17:48.577348Z","iopub.status.idle":"2024-08-24T20:17:48.593409Z","shell.execute_reply.started":"2024-08-24T20:17:48.577298Z","shell.execute_reply":"2024-08-24T20:17:48.592233Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                            sentence  \\\n0  RT @pzf : ECUADOR EARTHQUAKE : - At least 250 ...   \n1  RT @brk_news_now : FOX : ECUADOR ROCKED : Magn...   \n2  Independent : Video shows the moment earthquak...   \n3  RT @telesurenglish : Gracias Venezuela : Count...   \n4  RT @Emergency_Life : ·ºûA·ºû8·ºπ7 # Ecuador # Earthq...   \n\n                                         word_labels  \n0     O,O,O,U-CTRY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n1                 O,O,O,O,O,U-CTRY,O,O,O,O,O,O,O,O,O  \n2                   O,O,O,O,O,O,O,O,O,O,O,O,O,U-CTRY  \n3        O,O,O,O,U-CTRY,O,O,O,O,O,O,U-CTRY,O,O,O,O,O  \n4  O,O,O,O,O,U-CTRY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RT @pzf : ECUADOR EARTHQUAKE : - At least 250 ...</td>\n      <td>O,O,O,U-CTRY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RT @brk_news_now : FOX : ECUADOR ROCKED : Magn...</td>\n      <td>O,O,O,O,O,U-CTRY,O,O,O,O,O,O,O,O,O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Independent : Video shows the moment earthquak...</td>\n      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,U-CTRY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RT @telesurenglish : Gracias Venezuela : Count...</td>\n      <td>O,O,O,O,U-CTRY,O,O,O,O,O,O,U-CTRY,O,O,O,O,O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RT @Emergency_Life : ·ºûA·ºû8·ºπ7 # Ecuador # Earthq...</td>\n      <td>O,O,O,O,O,U-CTRY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Modeling preparation","metadata":{}},{"cell_type":"markdown","source":"## Prepare custom label mappings","metadata":{}},{"cell_type":"markdown","source":"Before fine-tuning, it‚Äôs essential to map the location mention labels from the BILOU format to a format that BERT can understand. This involves converting categorical labels (e.g., `B-CITY`,`B-CNTY`, `B-CONT`) into integer IDs, which the model will use during training. This mapping is critical because BERT outputs logits for each token, which are then converted back to these labels.","metadata":{}},{"cell_type":"code","source":"# Extract unique tags from word labels\ntags = set(\",\".join(data.word_labels).split(','))\n\n# Create label to ID and ID to label mappings\nlabel2id = {k: v for v, k in enumerate(tags)}\nid2label = {v: k for v, k in enumerate(tags)}","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:18:21.547576Z","iopub.execute_input":"2024-08-24T20:18:21.547998Z","iopub.status.idle":"2024-08-24T20:18:21.576325Z","shell.execute_reply.started":"2024-08-24T20:18:21.547959Z","shell.execute_reply":"2024-08-24T20:18:21.574717Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Setup the model and tokenizer","metadata":{}},{"cell_type":"code","source":"# Initialize the tokenizer using a pre-trained BERT model\ntokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n# Load a pre-trained BERT model for token classification with the custom label mappings\nmodel = BertForTokenClassification.from_pretrained(\n    \"bert-large-uncased\",\n    num_labels=len(id2label),\n    id2label=id2label,\n    label2id=label2id\n)\nmodel.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-24T20:18:23.202344Z","iopub.execute_input":"2024-08-24T20:18:23.203127Z","iopub.status.idle":"2024-08-24T20:18:38.704158Z","shell.execute_reply.started":"2024-08-24T20:18:23.203087Z","shell.execute_reply":"2024-08-24T20:18:38.703193Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e0fd8346db84b65a061d31357751852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a5598ce71f4abcbcdc32017bcdec4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72db22bcaf794936908c43e45bad63d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7c1a0c9e5c48928b6fcbf1c5d9bdd9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a485f845986140e593ef4e083db32c8a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n      (position_embeddings): Embedding(512, 1024)\n      (token_type_embeddings): Embedding(2, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=48, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"A custom dataset class is created to handle the input data, applying tokenization and ensuring that sequences are properly padded or truncated to fit the model‚Äôs expected input size. The `DataCollatorForTokenClassification` from the Hugging Face `transformers` library is used to dynamically pad batches during training, making the process efficient and preventing data leakage between samples.","metadata":{}},{"cell_type":"code","source":"# Initialize data collator for token classification\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:18:38.705704Z","iopub.execute_input":"2024-08-24T20:18:38.706028Z","iopub.status.idle":"2024-08-24T20:18:38.710265Z","shell.execute_reply.started":"2024-08-24T20:18:38.705995Z","shell.execute_reply":"2024-08-24T20:18:38.709299Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define maximum sequence length for tokenization\nMAX_LEN = 300\n\n# Create custom datasets for training and testing\ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:18:38.711339Z","iopub.execute_input":"2024-08-24T20:18:38.711674Z","iopub.status.idle":"2024-08-24T20:18:38.727856Z","shell.execute_reply.started":"2024-08-24T20:18:38.711641Z","shell.execute_reply":"2024-08-24T20:18:38.726736Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\nEPOCHS = 1 # Increase to 3 or more","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:18:41.468419Z","iopub.execute_input":"2024-08-24T20:18:41.469137Z","iopub.status.idle":"2024-08-24T20:18:41.473135Z","shell.execute_reply.started":"2024-08-24T20:18:41.469098Z","shell.execute_reply":"2024-08-24T20:18:41.472232Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The fine-tuning process involves setting up the `Trainer` class from the `transformers` library, which simplifies the training loop, handles model optimization, and tracks metrics like accuracy, precision, recall, and F1-score. We specify training arguments such as the number of epochs, batch size, learning rate, and the device (GPU or CPU). The model is trained to minimize the loss function, adjusting its weights based on the labeled data to improve its predictions.","metadata":{}},{"cell_type":"code","source":"# Set up training arguments for the Trainer API\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=VALID_BATCH_SIZE,\n    warmup_steps=25,\n    weight_decay=0.001,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    save_steps=50,\n    save_total_limit=2,\n    gradient_accumulation_steps=4,  # Accumulate gradients for larger effective batch size\n    fp16=True,  # Enable mixed precision training for faster computation\n    report_to=[\"none\"] #set this to true if you have a WANDB API key\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=training_set,\n    eval_dataset=testing_set,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics  # Function to compute metrics during evaluation\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:18:42.561382Z","iopub.execute_input":"2024-08-24T20:18:42.561777Z","iopub.status.idle":"2024-08-24T20:18:42.614824Z","shell.execute_reply.started":"2024-08-24T20:18:42.561738Z","shell.execute_reply":"2024-08-24T20:18:42.613895Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tuning","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"markdown","source":"For this task, we fine-tune the `BertForTokenClassification` model, a variant of BERT designed for sequence tagging tasks like Named Entity Recognition (NER). Fine-tuning involves taking a pre-trained BERT model and adapting it to our specific task‚Äîlocation mention recognition‚Äîby training it further on our labeled dataset. This step leverages the knowledge BERT has from its initial pre-training on a vast corpus while specializing it for identifying location mentions.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-24T20:18:44.572291Z","iopub.execute_input":"2024-08-24T20:18:44.572701Z","iopub.status.idle":"2024-08-24T20:43:53.791711Z","shell.execute_reply.started":"2024-08-24T20:18:44.572664Z","shell.execute_reply":"2024-08-24T20:43:53.790736Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [128/128 24:55, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>0.211500</td>\n      <td>0.072260</td>\n      <td>0.990000</td>\n      <td>0.985025</td>\n      <td>0.980100</td>\n      <td>0.990000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.034700</td>\n      <td>0.044535</td>\n      <td>0.990000</td>\n      <td>0.985025</td>\n      <td>0.980100</td>\n      <td>0.990000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.023400</td>\n      <td>0.043050</td>\n      <td>0.990000</td>\n      <td>0.987510</td>\n      <td>0.985034</td>\n      <td>0.990000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.019300</td>\n      <td>0.038477</td>\n      <td>0.990000</td>\n      <td>0.986683</td>\n      <td>0.983389</td>\n      <td>0.990000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.017500</td>\n      <td>0.034583</td>\n      <td>0.990000</td>\n      <td>0.987510</td>\n      <td>0.985034</td>\n      <td>0.990000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=128, training_loss=0.2722667141351849, metrics={'train_runtime': 1507.6093, 'train_samples_per_second': 10.909, 'train_steps_per_second': 0.085, 'total_flos': 8916977634508800.0, 'train_loss': 0.2722667141351849, 'epoch': 0.9961089494163424})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Measuring average WER","metadata":{}},{"cell_type":"markdown","source":"Now that the model is trained, let's make inference on train data to evaluate against the custom metric.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/lmd-competition/Train.csv\")\n\ntrain = train[~train['text'].isna()]","metadata":{"execution":{"iopub.status.busy":"2024-08-24T20:44:14.106896Z","iopub.execute_input":"2024-08-24T20:44:14.107604Z","iopub.status.idle":"2024-08-24T20:44:14.282321Z","shell.execute_reply.started":"2024-08-24T20:44:14.107565Z","shell.execute_reply":"2024-08-24T20:44:14.281258Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# implement in batches later\ntrain_predictions = infer_on_sentences(train.text.to_list(), trainer.model, tokenizer)\ntrain['prediction_raw'] = train_predictions","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-24T20:44:16.186326Z","iopub.execute_input":"2024-08-24T20:44:16.187165Z","iopub.status.idle":"2024-08-24T20:59:51.188976Z","shell.execute_reply.started":"2024-08-24T20:44:16.187127Z","shell.execute_reply":"2024-08-24T20:59:51.188018Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16448/16448 [15:34<00:00, 17.59it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df, average_wer = calculate_performance_metric(train, col2='prediction_raw')\naverage_wer","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:00:08.138493Z","iopub.execute_input":"2024-08-24T21:00:08.138886Z","iopub.status.idle":"2024-08-24T21:00:09.199464Z","shell.execute_reply.started":"2024-08-24T21:00:08.138848Z","shell.execute_reply":"2024-08-24T21:00:09.198568Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"1.308028741855043"},"metadata":{}}]},{"cell_type":"markdown","source":"Can we do better?\n\nHere we will perform some post-inference cleaning,using the `clean_prediction` function defined in the helpers section. Nothing fancy, just a bunch of heuristics.","metadata":{}},{"cell_type":"code","source":"train['prediction_clean'] = train.apply(clean_prediction, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:00:11.465929Z","iopub.execute_input":"2024-08-24T21:00:11.466696Z","iopub.status.idle":"2024-08-24T21:00:12.047362Z","shell.execute_reply.started":"2024-08-24T21:00:11.466650Z","shell.execute_reply":"2024-08-24T21:00:12.046583Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df, average_wer = calculate_performance_metric(train, col2='prediction_clean')\naverage_wer","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:00:12.356888Z","iopub.execute_input":"2024-08-24T21:00:12.357275Z","iopub.status.idle":"2024-08-24T21:00:13.297746Z","shell.execute_reply.started":"2024-08-24T21:00:12.357238Z","shell.execute_reply":"2024-08-24T21:00:13.296782Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.6076609679433731"},"metadata":{}}]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/lmd-competition/Test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:19:08.531354Z","iopub.execute_input":"2024-08-24T21:19:08.532078Z","iopub.status.idle":"2024-08-24T21:19:08.557247Z","shell.execute_reply.started":"2024-08-24T21:19:08.532038Z","shell.execute_reply":"2024-08-24T21:19:08.556439Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# implement in batches later\ntest_predictions = infer_on_sentences(test.text.to_list(), model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:19:13.947927Z","iopub.execute_input":"2024-08-24T21:19:13.948319Z","iopub.status.idle":"2024-08-24T21:22:00.333018Z","shell.execute_reply.started":"2024-08-24T21:19:13.948279Z","shell.execute_reply":"2024-08-24T21:22:00.332058Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2942/2942 [02:46<00:00, 17.68it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"test['prediction_raw'] = test_predictions\ntest['prediction'] = test.apply(clean_prediction, axis=1)\ntest['prediction'] = test['prediction'].replace(\"\", \" \")","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:22:00.335017Z","iopub.execute_input":"2024-08-24T21:22:00.335800Z","iopub.status.idle":"2024-08-24T21:22:00.449767Z","shell.execute_reply.started":"2024-08-24T21:22:00.335749Z","shell.execute_reply":"2024-08-24T21:22:00.448704Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test[['tweet_id', 'prediction']].to_csv(\"bert-large-uncased-fine-tuned-1-epoch+huristic-cleaning.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:22:28.767289Z","iopub.execute_input":"2024-08-24T21:22:28.768050Z","iopub.status.idle":"2024-08-24T21:22:28.780819Z","shell.execute_reply.started":"2024-08-24T21:22:28.768011Z","shell.execute_reply":"2024-08-24T21:22:28.779920Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Some improvement ideas\n- Obviously, improving the fine-tuning for better results\n- Fine tunning other models\n- using the location hierarchy to re-order output\n- cleaning tweets and predictions using a grammar correction model such as t5-base-grammar-correction\n- ensemble modeling...","metadata":{}},{"cell_type":"markdown","source":"# Extra - pushing the model into the hub ","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:05:21.374690Z","iopub.execute_input":"2024-08-24T21:05:21.375086Z","iopub.status.idle":"2024-08-24T21:05:21.405814Z","shell.execute_reply.started":"2024-08-24T21:05:21.375049Z","shell.execute_reply":"2024-08-24T21:05:21.404785Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04389a8d8e444081812546e7df81b318"}},"metadata":{}}]},{"cell_type":"code","source":"#  Push the fine-tuned model and tokenizer to the Hub\ntrainer.model.push_to_hub(\"Sifr-un/bert-large-uncased-LMD-base\")\ntrainer.tokenizer.push_to_hub(\"Sifr-un/bert-large-uncased-LMD-base\")","metadata":{"execution":{"iopub.status.busy":"2024-08-24T21:05:42.081808Z","iopub.execute_input":"2024-08-24T21:05:42.082640Z","iopub.status.idle":"2024-08-24T21:06:19.455489Z","shell.execute_reply.started":"2024-08-24T21:05:42.082600Z","shell.execute_reply":"2024-08-24T21:06:19.454455Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a089eb506b4a4a108c6d8114a62345ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d2e0e2f53e54f3781cabca16d3106c0"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Sifr-un/bert-large-uncased-LMD-base/commit/4f8aa55a4f1aaa59421579b0676ef2d855052821', commit_message='Upload tokenizer', commit_description='', oid='4f8aa55a4f1aaa59421579b0676ef2d855052821', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}